{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoConfig, BertConfig, BertTokenizer, \n",
    "    BertForSequenceClassification)\n",
    "    \n",
    "import sys   \n",
    "sys.path.insert(1, \"/home/sarahwooders_gmail_com/transformers/\")\n",
    "from src.transformers.modeling_bert import BertForRetrieval\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_dir = \"/home/sarahwooders_gmail_com/transformers/checkpoints/checkpoint-28000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(ckpt_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    ckpt_dir,\n",
    "    do_lower_case=True,\n",
    ")\n",
    "\n",
    "model = BertForRetrieval.from_pretrained(\n",
    "    ckpt_dir,\n",
    "    from_tf=False,\n",
    "    #config=config,\n",
    "    #cache_dir='/home/sarahwooders_gmail_com/transformers/checkpoints'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "use_gpu = False\n",
    "if use_gpu:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_distance(text_a, text_b):\n",
    "\n",
    "    max_seq_length=128\n",
    "    test_input_a = tokenizer.batch_encode_plus(\n",
    "        [(text_a, None)],\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=True,\n",
    "    )\n",
    "\n",
    "    test_input_b = tokenizer.batch_encode_plus(\n",
    "        [(text_b, None)],\n",
    "        max_length=max_seq_length,\n",
    "        pad_to_max_length=True,\n",
    "    )\n",
    "\n",
    "    model_inp_a = dict(test_input_a)\n",
    "    model_inp_b = dict(test_input_b)\n",
    "    model_inp_a['labels'] = [0]\n",
    "    model_inp_b['labels'] = [0]\n",
    "    model_inp_a = {k: torch.tensor(v).to(device) for k, v in model_inp_a.items()}\n",
    "    model_inp_b = {k: torch.tensor(v).to(device) for k, v in model_inp_b.items()}\n",
    "\n",
    "    model_out = model(\n",
    "        input_ids_a=model_inp_a['input_ids'], \n",
    "        attention_mask_a=model_inp_a['attention_mask'],\n",
    "        token_type_ids_a=model_inp_a['token_type_ids'],\n",
    "        input_ids_b=model_inp_b['input_ids'], \n",
    "        attention_mask_b=model_inp_b['attention_mask'],\n",
    "        token_type_ids_b=model_inp_b['token_type_ids']\n",
    "    )\n",
    "\n",
    "    title_embedding = model_out[1].detach().cpu().numpy()[0]\n",
    "    category_embedding = model_out[2].detach().cpu().numpy()[0]\n",
    "\n",
    "    t = title_embedding/np.linalg.norm(title_embedding)\n",
    "    c = category_embedding/np.linalg.norm(category_embedding)\n",
    "    return np.dot(t, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "\"Juice Del Valle Frut Pet Peach 1 L Juice Del Valle Frut Pet Peach 1 L\",\n",
    "\"Elseve Total 5 Extra Professional Serum 15 mL Elseve Total 5 Extra Professional Serum 15 mL\",\n",
    "\"Corbalan Canned Mixed Skewer 500 g Corbalan Canned Mixed Skewer 500 g\",\n",
    "\"Pedigree Dog Food Puppies Medium and Large Breeds 3 kg With Vitamins and Minerals that help you stay strong and healthy. With Natural Fibers for optimal digestion. With Calcium for growth. With protein for strong muscles. Textured grains that help reduce the formation of tartar and keep your teeth and gums healthy.\",\n",
    "\"Kendall 7/8 BC Sock Without Tip M 1322 Kendall 7/8 BC Sock Without Tip M 1322\",\n",
    "\"Johnsons Roma Soap 80 g Johnsons Roma Soap 80 g\",\n",
    "\"Sensodyne Extra Fresh Rinse 250 ml Sensodyne Extra Fresh Rinse 250 ml\",\n",
    "\"Kelloggs Granola And Honey Biscuit 120 Gr Kelloggs Granola And Honey Biscuit 120 Gr\",\n",
    "\"Multivitaminico Vita Force AZ 120 Cáps - Voxx Multivitaminico Vita Force AZ 120 Cáps - Voxx\",\n",
    "\"Bioext Queravit Megadose 15 mL Bioext Queravit Megadose 15 mL\",\n",
    "\"Moo Yogurt Type Skyr Coco Without Lactose Moo Yogurt Type Skyr Coco Without Lactose\",\n",
    "\"Beer 600 ml Choose your beer.\",\n",
    "\"Bienn 4mg 10 Tablets Bienn 4Mg 10 Tablets\",\n",
    "\"Dental Brush B Indicator Nº40 2 U Dental Brush Indicator Oral-B, White Teeth And Healthy Gums. It has bristles with rounded tips and with Indicator Plus system, which indicates the moment of brush change, its handle is rubberized with comfort grip, which provides more safety and comfort during brushing. Enjoy the Promotion Take 2, Pay 1. Product with colors and / or assorted prints. Shipping According to Stock Availability. (Key Words: Toothpaste, Toothpaste, Oral Health, Oral Hygiene, Anticaries, Toothbrush, Tooth Hiding)\",\n",
    "\"Nestlé Flan Strawberry Dessert Set 200 g Nestlé Flan Strawberry Dessert Set 200 g (Key Words: Dairy / Morning)\",\n",
    "\"Wickbold Scooby Doo Integral Tube 300 g The Wickbold Scooby Doo wholegrain biscuit was made with wholemeal flour especially for those who follow a slimming diet and cannot do without the daily bisnaguinha. Healthy, soft and tasty, ideal for any time of the day.\",\n",
    "\"Orthodontic Success Interdental Kit x 4 Units - PLU: 33455\",\n",
    "\"Dark Chocolate With Tiramisu Kopenhagen Filling Dark Chocolate With Tiramisu Kopenhagen Filling\",\n",
    "\"Naturafrig Bovine Palette Bovine Naturafrig Bulk Palette\",\n",
    "\"Palmolive Naturals Soap Secret Seductive 90 g Palmolive Naturals Soap Secret Seductive 90 g. (Key Words: Body Care, Beauty Care, Beauty, Hygiene, Deodorant, Anti Perspirant, Antiperspirant, Soap, Soap, Bar Soap)\",\n",
    "]\n",
    "\n",
    "categories = [\n",
    "    \"food\",\n",
    "    \"health and beauty\", \n",
    "    \"cleaning supplies\", \n",
    "    \"canned foods\", \n",
    "    \"pet supplies\", \n",
    "    \"fashion apparel\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-156-d585f0080adc>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-156-d585f0080adc>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    #print()\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for t in test:\n",
    "    print(t)\n",
    "    for c in categories:\n",
    "        print(\"\\t\", embedding_distance(t, c), c)\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rappi data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "labels = []\n",
    "samples = []\n",
    "\n",
    "# took out subcategory or else evaluation takes forever\n",
    "\n",
    "with open(\"rappi_sub_categories.csv\", newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        #labels.append(row['category'] + ' ' + row['subcategory'])\n",
    "        labels.append(row['category'])\n",
    "labels = list(set(labels))\n",
    "\n",
    "with open(\"rappi_human.csv\", newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter=',')\n",
    "    for row in reader:\n",
    "        text = row['translated_title'] + ' ' + row['translated_description']\n",
    "        label = row['category'] #+ ' ' + row['subcategory']\n",
    "        if label not in labels:\n",
    "            continue\n",
    "            \n",
    "        samples.append((text, label))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1125\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "print(len(samples))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/182 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Japanese Mafer Premium Roasted Peanuts with Lemon 180 g Japanese Mafer Premium Roasted Peanuts with Lemon 180 g - Prices including VAT', 'snacks and confectionery')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 182/182 [00:42<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.75331056, 'office supplies'), (0.7533928, 'medical equipment and supplies'), (0.75372857, 'school supplies'), (0.75402176, 'electronic supplies'), (0.75913936, 'cards')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in samples: \n",
    "    print(sample)\n",
    "    predictions = [(embedding_distance(sample[0], l), l) for l in tqdm(labels)]\n",
    "    predictions = sorted(predictions)\n",
    "    print(predictions[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
