To ssh into Google cloud
ssh -i ~/.ssh/google_compute_engine uma_roy_us_gmail_com@35.197.21.166

https://docs.python-guide.org/dev/virtualenvs/
pipenv commands

make sure ssh-agent is running?

sshfs -o IdentityFile=~/.ssh/google_compute_engine uma_roy_us_gmail_com@35.197.13.32:/home/uma_roy_us_gmail_com gcp/

pyenv to manage python versions
pipenv to manage installed packages
pipenv shell

Things to install:
transformers (local) (pipenv install '-e .')
scikit-learn
torch, torchvision
tensorboard
bson # for loading doordash pickle


python examples/text-classification/run_xnli.py \
  --data_dir="~/Documents/Programming/transformers/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --output_dir="/tmp/retrieval-debug/" \
  --language="en" \
  --do_train


python examples/text-retrieval/run_retrieval.py \
  --data_dir="/Users/umaroy/Documents/Programming/transformers/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --output_dir="/tmp/retrieval-debug-2/" \
  --do_train

python examples/text-retrieval/run_retrieval.py \
--data_dir="/Users/umaroy/Documents/Programming/transformers/data_dir/" \
--model_name_or_path="bert-base-multilingual-cased" \
--output_dir="/tmp/retrieval-debug/" \
--do_eval

python examples/text-retrieval/run_retrieval.py \
--data_dir="/Users/umaroy/Documents/Programming/transformers/data_dir/" \
--model_name_or_path="bert-base-multilingual-cased" \
--run_name="test_run" \
--do_eval

scp -i ~/.ssh/google_compute_engine data_dir/cached_train_bert-base-multilingual-cased_128_doordash_cuisine uma_roy_us_gmail_com@35.197.21.166:/home/uma_roy_us_gmail_com/transformers/data_dir
scp -i ~/.ssh/google_compute_engine data_dir/cached_train_bert-base-multilingual-cased_128_doordash_title uma_roy_us_gmail_com@35.197.21.166:/home/uma_roy_us_gmail_com/transformers/data_dir
scp -i ~/.ssh/google_compute_engine data_dir/doordash_categorized.pkl uma_roy_us_gmail_com@35.197.21.166:/home/uma_roy_us_gmail_com/transformers/data_dir

cached_train_bert-base-multilingual-cased_128_doordash_title
doordash_categorized.pkl

On remote machine:
python examples/text-retrieval/run_retrieval.py \
  --data_dir="/home/uma_roy_us_gmail_com/transformers/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --run_name="batch-32_max-seq-len-55_noother" \
  --max_seq_length=55 \
  --per_gpu_train_batch_size=32 \
  --do_train \
  --logging_steps=100 \
  --overwrite_cache

scp -i ~/.ssh/google_compute_engine data_dir/XNLI-1.0 uma_roy_us_gmail_com@35.197.21.166:/home/uma_roy_us_gmail_com/transformers/data_dir
scp -i ~/.ssh/google_compute_engine data_dir/ uma_roy_us_gmail_com@35.197.21.166:/home/uma_roy_us_gmail_com/transformers/data_dir


python examples/text-classification/run_glue.py --model_name_or_path=bert-base-multilingual-cased --do_train --task_name=sts-b --data_dir=glue_data/STS-B  --output_dir=tmp/sts-b-test

How to rebase upstream into forked repo
-- https://medium.com/@topspinj/how-to-git-rebase-into-a-forked-repo-c9f05e821c8a

ssh -i ~/.ssh/google_compute_engine -R 6006:localhost:6006 uma_roy_us_gmail_com@35.197.21.166

For jupyter notebook port forwarding
gcloud compute ssh nvidia-gpu-cloud-pytorch-image-1-vm \
    --project vast-alcove-231918 \
    --zone us-west1-b \
    -- -L 2222:localhost:8888


gcloud compute ssh nvidia-gpu-cloud-pytorch-image-1-vm \
    --project vast-alcove-231918 \
    --zone us-west1-b \
    -- -L 2222:localhost:6006

ssh glisten (~/.ssh/config)

To install pyenv and pipenv:
https://github.com/pyenv/pyenv/wiki/Common-build-problems
pyenv install 3.7.7
switch to relevant python version into pyenv,
then pip install pipenv
Then reload shell and pipenv should work...


6/5/2020:
Run classification pipeline
python examples/text-classification/run_doordash.py \
  --data_dir="/home/uma_roy_us_gmail_com/transformers/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --run_name="classification_test" \
  --max_seq_length=55 \
  --per_gpu_train_batch_size=32 \
  --do_train \
  --logging_steps=5


python examples/text-classification/run_doordash.py \
  --data_dir="/home/uma_roy_us_gmail_com/transformers/data_dir/" \
  --model_name_or_path="xlm-roberta-large" \
  --run_name="classification_32_55_xlm_roberta" \
  --max_seq_length=55 \
  --per_gpu_train_batch_size=32 \
  --do_train \
  --logging_steps=100

For classification:
--train on all labels and note best performance: seems like ~50%

--Get eval mode to dump all encodings ... what size? 
--look at PCA or tSNE and examine what clusters look like 
--suspect there will be problem clusters that overlap that are confusing classifier
--implement a per-class accuracy to get sense of how individual classes are doing
--weight class by class-imblanace (inverse)?
--Also can plug in different models (Roberta, etc.)

2-tiered classifier:
--**train classifier to train "other" vs. desired categories
--**then another classifier to distinguish between desired categories
^ look at output of the above manually because that is what they care about...

Co-opting evaluation to dump the encodings:
python examples/text-classification/run_doordash.py \
  --data_dir="/home/uma_roy_us_gmail_com/transformers/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --run_name="classification_test/checkpoint-4500" \
  --max_seq_length=55 \
  --do_eval \
  --per_gpu_eval_batch_size=128


  scp -i ~/.ssh/google_compute_engine doordash_test.csv uma_roy_us_gmail_com@35.197.21.166:/home/uma_roy_us_gmail_com/transformers/data_dir


Setting up use of this repo:

* Install pyenv
* Modify bashrc to have shims
* Make sure pyenv works
* Install 3.7.7
* Install pipenv (while you are in the version of pyenv)
* Clone transformers repo (this repo)
* pipenv install--should install transformers environment based on Pipfile in git
* pipenv shell (gives you (transformers) prompt in your command line)
* https://anbasile.github.io/posts/2017-06-25-jupyter-venv/ (look at tl;dr)
* launch jupyter notebook from the environment in the transformers directory
* Go to localhost:8888 and you should see the jupyter instance running

cat ~/.ssh/config

Host glisten
	HostName 35.197.21.166
	User uma_roy_us_gmail_com
	IdentityFile ~/.ssh/google_compute_engine
	LocalForward 8888 localhost:8888
	LocalForward 8889 localhost:8889
	LocalForward 6006 localhost:6006
	LocalForward 6007 localhost:6007


TODO 5/8:
Cuisine type:
--run current classification model with just given labels,
 then use model to label asian, etc. to get cleaned data, 
 then re-train model with silver data (lol self-distillation)
 --train model with description (and maybe section)

Dish type:
--clean data by 'section' to reflect pizza, etc. to turn into classification problem 
("other" is a big problem)
--train dish type classification model on cleaned data and test it!

Devops:
Clean up output dir and log dir folder at some point
DONE: add wandb tracking

python examples/text-classification/run_doordash.py \
  --data_dir="/home/transformers-public/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --run_name="wandb_test" \
  --max_seq_length=55 \
  --per_gpu_train_batch_size=32 \
  --do_train \
  --logging_steps=100 \
  --writing_dir="/home/transformers-public" \
  --max_steps=100

preprocessing
doordash text --> (title, description) pairs
--filtered by relevant cuisine types
--concat all together


Simplest MVP:
doordash text --> (title, description)
feed through net                                                                                                                                                                                                                                                                                                                                                                                         



List[InputFeatures]
f.input_ids, t.token_type_ids, etc.


python examples/text-classification/run_doordash.py \
  --data_dir="/home/transformers-public/data_dir/" \
  --model_name_or_path="bert-base-multilingual-cased" \
  --run_name="test_new_cleaned_loader" \
  --max_seq_length=64 \
  --per_gpu_train_batch_size=32 \
  --do_train \
  --logging_steps=100 \
  --writing_dir="/home/transformers-public" \
  --max_steps=100000